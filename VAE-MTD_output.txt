----------------- Options ---------------
               batch_size: 50                            
            choose_best_k: False                         
                   epochs: 150                           	[default: 100]
                     file: Data/imputed_SweatBinary.csv  	[default: ../Data/imputed_SweatBinary.csv]
                  gpu_ids: 0                             
                        k: 3                             
                  num_obs: 100                           
                 set_seed: 1                             
----------------- End -------------------
https://app.neptune.ai/jaysivakumar/VAE-MTD/e/VAEM1-2
Encoder(
  (seq): Sequential(
    (0): Linear(in_features=23, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
  )
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=128, bias=True)
)
Decoder(
  (seq): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=23, bias=True)
  )
)
Discriminator(
  (seq): Sequential(
    (0): Linear(in_features=23, out_features=128, bias=True)
    (1): LeakyReLU(negative_slope=0.2)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=128, out_features=128, bias=True)
    (4): LeakyReLU(negative_slope=0.2)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=128, out_features=1, bias=True)
  )
)
Epoch 1, Loss G:  4.3882, Loss D: -2.9528
Epoch 2, Loss G:  3.9463, Loss D: -3.0119
Epoch 3, Loss G:  3.8398, Loss D: -3.1073
Epoch 4, Loss G:  3.6465, Loss D: -2.9891
Epoch 5, Loss G:  3.4939, Loss D: -2.9717
Epoch 6, Loss G:  3.4243, Loss D: -3.0834
Epoch 7, Loss G:  3.4527, Loss D: -3.0571
Epoch 8, Loss G:  3.2333, Loss D: -2.8834
Epoch 9, Loss G:  3.4784, Loss D: -2.8424
Epoch 10, Loss G:  3.3147, Loss D: -3.1636
Epoch 11, Loss G:  3.5047, Loss D: -2.9578
Epoch 12, Loss G:  3.5653, Loss D: -3.3571
Epoch 13, Loss G:  3.8020, Loss D: -3.1662
Epoch 14, Loss G:  3.7167, Loss D: -3.1738
Epoch 15, Loss G:  3.4914, Loss D: -2.9815
Epoch 16, Loss G:  3.5665, Loss D: -3.2488
Epoch 17, Loss G:  3.7147, Loss D: -2.8333
Epoch 18, Loss G:  3.8539, Loss D: -3.1294
Epoch 19, Loss G:  3.7093, Loss D: -3.1979
Epoch 20, Loss G:  3.8521, Loss D: -3.2875
Epoch 21, Loss G:  3.9806, Loss D: -3.4491
Epoch 22, Loss G:  3.7863, Loss D: -3.4317
Epoch 23, Loss G:  3.7959, Loss D: -3.2389
Epoch 24, Loss G:  3.8906, Loss D: -3.2454
Epoch 25, Loss G:  3.9466, Loss D: -3.4056
Epoch 26, Loss G:  3.8598, Loss D: -3.2015
Epoch 27, Loss G:  4.0471, Loss D: -3.2082
Epoch 28, Loss G:  4.1624, Loss D: -3.1302
Epoch 29, Loss G:  4.0031, Loss D: -3.1038
Epoch 30, Loss G:  4.0906, Loss D: -3.1385
Epoch 31, Loss G:  4.3891, Loss D: -3.3519
Epoch 32, Loss G:  4.0961, Loss D: -3.2711
Epoch 33, Loss G:  4.3047, Loss D: -3.3262
Epoch 34, Loss G:  4.2309, Loss D: -3.1371
Epoch 35, Loss G:  4.1885, Loss D: -3.1390
Epoch 36, Loss G:  4.3668, Loss D: -3.0574
Epoch 37, Loss G:  4.3289, Loss D: -3.0198
Epoch 38, Loss G:  4.2490, Loss D: -3.2049
Epoch 39, Loss G:  4.1262, Loss D: -3.1969
Epoch 40, Loss G:  4.1401, Loss D: -3.1665
Epoch 41, Loss G:  4.3473, Loss D: -3.3234
Epoch 42, Loss G:  4.3878, Loss D: -3.4434
Epoch 43, Loss G:  4.3386, Loss D: -3.2340
Epoch 44, Loss G:  4.1983, Loss D: -3.1286
Epoch 45, Loss G:  4.3757, Loss D: -3.2382
Epoch 46, Loss G:  4.3361, Loss D: -3.1094
Epoch 47, Loss G:  4.2696, Loss D: -3.3032
Epoch 48, Loss G:  4.4411, Loss D: -3.2723
Epoch 49, Loss G:  4.3479, Loss D: -3.2911
Epoch 50, Loss G:  4.1936, Loss D: -3.2056
Epoch 51, Loss G:  4.4666, Loss D: -3.4150
Epoch 52, Loss G:  4.3657, Loss D: -3.4509
Epoch 53, Loss G:  4.4303, Loss D: -3.4785
Epoch 54, Loss G:  4.4819, Loss D: -3.1196
Epoch 55, Loss G:  4.3782, Loss D: -3.1818
Epoch 56, Loss G:  4.4782, Loss D: -3.1857
Epoch 57, Loss G:  4.4626, Loss D: -2.9806
Epoch 58, Loss G:  4.6003, Loss D: -3.4174
Epoch 59, Loss G:  4.5372, Loss D: -2.9925
Epoch 60, Loss G:  4.5677, Loss D: -3.2451
Epoch 61, Loss G:  4.6338, Loss D: -3.3235
Epoch 62, Loss G:  4.6887, Loss D: -3.1721
Epoch 63, Loss G:  4.8112, Loss D: -3.4735
Epoch 64, Loss G:  4.9092, Loss D: -3.2359
Epoch 65, Loss G:  4.9455, Loss D: -3.2673
Epoch 66, Loss G:  4.5851, Loss D: -3.2233
Epoch 67, Loss G:  4.6615, Loss D: -3.4965
Epoch 68, Loss G:  5.0273, Loss D: -3.2672
Epoch 69, Loss G:  4.8377, Loss D: -3.3107
Epoch 70, Loss G:  4.9785, Loss D: -3.2517
Epoch 71, Loss G:  4.8431, Loss D: -3.1005
Epoch 72, Loss G:  4.6131, Loss D: -3.1539
Epoch 73, Loss G:  4.7497, Loss D: -3.3447
Epoch 74, Loss G:  4.7712, Loss D: -2.9362
Epoch 75, Loss G:  4.8426, Loss D: -3.4364
Epoch 76, Loss G:  4.6353, Loss D: -3.3825
Epoch 77, Loss G:  4.5699, Loss D: -3.3760
Epoch 78, Loss G:  4.5297, Loss D: -3.0824
Epoch 79, Loss G:  4.7793, Loss D: -3.0258
Epoch 80, Loss G:  4.9053, Loss D: -3.2602
Epoch 81, Loss G:  4.3452, Loss D: -3.5627
Epoch 82, Loss G:  5.0904, Loss D: -3.2376
Epoch 83, Loss G:  4.9027, Loss D: -3.2639
Epoch 84, Loss G:  4.8696, Loss D: -2.9478
Epoch 85, Loss G:  4.7371, Loss D: -3.2455
Epoch 86, Loss G:  4.5833, Loss D: -3.1880
Epoch 87, Loss G:  4.6411, Loss D: -3.0296
Epoch 88, Loss G:  4.6218, Loss D: -3.2040
Epoch 89, Loss G:  4.6910, Loss D: -3.3515
Epoch 90, Loss G:  4.8441, Loss D: -3.3013
Epoch 91, Loss G:  4.5210, Loss D: -3.3275
Epoch 92, Loss G:  4.7109, Loss D: -3.2161
Epoch 93, Loss G:  4.6914, Loss D: -3.2979
Epoch 94, Loss G:  4.4609, Loss D: -3.3084
Epoch 95, Loss G:  4.5285, Loss D: -3.3095
Epoch 96, Loss G:  4.5623, Loss D: -3.4176
Epoch 97, Loss G:  4.5530, Loss D: -3.0858
Epoch 98, Loss G:  4.7476, Loss D: -3.2792
Epoch 99, Loss G:  4.7738, Loss D: -3.4431
Epoch 100, Loss G:  4.6086, Loss D: -3.0811
Epoch 101, Loss G:  4.6228, Loss D: -3.1525
Epoch 102, Loss G:  4.6898, Loss D: -3.2148
Epoch 103, Loss G:  4.4419, Loss D: -3.1869
Epoch 104, Loss G:  4.2669, Loss D: -3.0702
Epoch 105, Loss G:  4.6126, Loss D: -3.2671
Epoch 106, Loss G:  4.5185, Loss D: -3.3068
Epoch 107, Loss G:  4.5271, Loss D: -3.2741
Epoch 108, Loss G:  4.6401, Loss D: -3.3758
Epoch 109, Loss G:  4.5461, Loss D: -3.4018
Epoch 110, Loss G:  4.4817, Loss D: -3.3472
Epoch 111, Loss G:  4.5976, Loss D: -3.3646
Epoch 112, Loss G:  4.6911, Loss D: -3.1952
Epoch 113, Loss G:  4.4936, Loss D: -3.2673
Epoch 114, Loss G:  4.4212, Loss D: -3.2731
Epoch 115, Loss G:  4.3280, Loss D: -3.1371
Epoch 116, Loss G:  4.1158, Loss D: -2.9307
Epoch 117, Loss G:  4.1171, Loss D: -3.6145
Epoch 118, Loss G:  4.2244, Loss D: -3.2481
Epoch 119, Loss G:  4.3345, Loss D: -2.9953
Epoch 120, Loss G:  4.6754, Loss D: -3.0214
Epoch 121, Loss G:  4.4934, Loss D: -3.1622
Epoch 122, Loss G:  3.9768, Loss D: -3.4267
Epoch 123, Loss G:  4.5210, Loss D: -3.3137
Epoch 124, Loss G:  4.4622, Loss D: -3.1356
Epoch 125, Loss G:  4.4864, Loss D: -3.2474
Epoch 126, Loss G:  4.2346, Loss D: -3.4464
Epoch 127, Loss G:  4.4946, Loss D: -3.1946
Epoch 128, Loss G:  4.3402, Loss D: -3.0184
Epoch 129, Loss G:  4.7292, Loss D: -3.5006
Epoch 130, Loss G:  4.7669, Loss D: -2.9999
Epoch 131, Loss G:  4.3490, Loss D: -3.1550
Epoch 132, Loss G:  4.7628, Loss D: -3.2026
Epoch 133, Loss G:  4.7695, Loss D: -3.2432
Epoch 134, Loss G:  4.7506, Loss D: -3.0993
Epoch 135, Loss G:  4.6540, Loss D: -3.2880
Epoch 136, Loss G:  4.6271, Loss D: -3.4340
Epoch 137, Loss G:  4.3181, Loss D: -3.1813
Epoch 138, Loss G:  4.3650, Loss D: -3.2510
Epoch 139, Loss G:  4.7195, Loss D: -3.1530
Epoch 140, Loss G:  4.2891, Loss D: -2.9593
Epoch 141, Loss G:  4.1295, Loss D: -3.4392
Epoch 142, Loss G:  4.4695, Loss D: -3.3575
Epoch 143, Loss G:  4.4407, Loss D: -3.0214
Epoch 144, Loss G:  4.4811, Loss D: -3.2147
Epoch 145, Loss G:  4.2792, Loss D: -3.0425
Epoch 146, Loss G:  4.4992, Loss D: -3.1009
Epoch 147, Loss G:  4.3885, Loss D: -3.2483
Epoch 148, Loss G:  4.7371, Loss D: -3.2207
Epoch 149, Loss G:  4.1035, Loss D: -3.1248
Epoch 150, Loss G:  4.2078, Loss D: -3.2520
Shutting down background jobs, please wait a moment...
Done!
All 332875 operations synced, thanks for waiting!
